{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cb910-ef1b-4da7-8e15-5907928b1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib.parse\n",
    "import base64\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load a CSV file containing the SQL injection dataset.\n",
    "# The file is assumed to be named \"Modified_SQL_Dataset.csv\".\n",
    "# A random sample of size 10,000 is taken from the dataset for processing.\n",
    "df = pd.read_csv(\"Modified_SQL_Dataset.csv\").sample(10000).reset_index(drop=True)\n",
    "\n",
    "# Define a function to decode encoded SQL queries.\n",
    "def decode_sql(encoded_string):\n",
    "    decoded_string = encoded_string\n",
    "    try:\n",
    "        # Try different decoding methods to get the original query.\n",
    "        decoded_string = bytes.fromhex(encoded_string).decode('ascii')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        decoded_string = bytes.fromhex(encoded_string).decode('unicode_escape')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        decoded_string = json.loads(encoded_string)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        decoded_string = urllib.parse.unquote(encoded_string)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        decoded_string = base64.b64decode(encoded_string).decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    return decoded_string\n",
    "\n",
    "# Define a function to convert SQL queries to lower case.\n",
    "def lowercase_sql(query):\n",
    "    return query.lower()\n",
    "\n",
    "# Define a function to replace digits in the SQL query with a generic 0.\n",
    "def generalize_sql(query):\n",
    "    generalized_query = re.sub(r'\\d+', '0', query)\n",
    "    return generalized_query\n",
    "\n",
    "# Define a function to tokenize the SQL query.\n",
    "def tokenize_sql(query):\n",
    "    query = re.sub(r'([<>!=])', r' \\1 ', query)  # Separate operators with spaces.\n",
    "    tokens = query.split()\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the 'Query' column in the dataframe using the defined functions.\n",
    "df['Text'] = df['Query'].apply(decode_sql)\n",
    "df['Text'] = df['Text'].apply(lowercase_sql)\n",
    "df['Text'] = df['Text'].apply(generalize_sql)\n",
    "df['Text'] = df['Text'].apply(tokenize_sql)\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, random_state=50, shuffle=True)\n",
    "train_texts, train_labels = train_df['Text'].tolist(), train_df['Label'].tolist()\n",
    "test_texts, test_labels = test_df['Text'].tolist(), test_df['Label'].tolist()\n",
    "\n",
    "# Custom Dataset class to handle data loading.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define a BERT-based classifier with a convolutional neural network (CNN) architecture.\n",
    "class BertTextCNNClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_filters, filter_sizes, output_size):\n",
    "        super(BertTextCNNClassifier, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        # Define convolutional layers.\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=bert_model.config.hidden_size, out_channels=num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedded = outputs.last_hidden_state.transpose(1, 2)\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for conv_layer in self.conv_layers:\n",
    "            conv_out = nn.functional.relu(conv_layer(embedded))\n",
    "            pooled_out, _ = torch.max(conv_out, dim=2)\n",
    "            pooled_outputs.append(pooled_out)\n",
    "\n",
    "        # Concatenate pooled outputs and flatten.\n",
    "        pooled_outputs = torch.cat(pooled_outputs, dim=1)\n",
    "        pooled_outputs = self.dropout(pooled_outputs)\n",
    "\n",
    "        # Fully connected layer for classification.\n",
    "        logits = self.fc(pooled_outputs)\n",
    "        return logits\n",
    "\n",
    "# Define parameters for training.\n",
    "batch_size = 64\n",
    "max_length = 128\n",
    "output_size = 2  # Number of classes in your classification task\n",
    "\n",
    "# Initialize the BERT tokenizer and model.\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Setup data loaders.\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = CustomDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define model, loss criterion, and optimizer.\n",
    "num_filters = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "model = BertTextCNNClassifier(bert_model, num_filters, filter_sizes, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Use a GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Begin training loop.\n",
    "print(\"Start training\")\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\") as t:\n",
    "        for batch in t:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            t.set_postfix({'loss': total_loss / (t.n + 1), 'accuracy': correct_train / total_train})\n",
    "\n",
    "# Evaluate the model with the test dataset.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with tqdm(test_loader, unit=\"batch\") as t:\n",
    "        for batch in t:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score.\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print evaluation metrics.\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvt",
   "language": "python",
   "name": "venvt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
